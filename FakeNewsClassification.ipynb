{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FakeNewsClassification.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aafreen2212/DL/blob/master/FakeNewsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LUX0LaYmKNa"
      },
      "source": [
        "# TASK #1: UNDERSTAND THE PROBLEM STATMENT AND BUSINESS CASE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSB8ry4EJ1WM"
      },
      "source": [
        "Welcome to \"NLP : Fake News Detector\". This is a project-based course which should take approximately 1.5 hours to finish. Before diving into the project, please take a look at the course objectives and structure:\n",
        "Course Objectives\n",
        "\n",
        "In this course, we are going to focus on the following learning objectives:\n",
        "\n",
        "   1. Apply python libraries to import and visualize datasets\n",
        "  2.  Perform exploratory data analysis and plot word-cloud\n",
        "  3.  Perform text data cleaning such as removing punctuation and stop words\n",
        "  4.  Understand the concept of tokenizer.\n",
        " 5.   Perform tokenizing and padding on text corpus to feed the deep learning model.\n",
        "  6.  Understand the theory and intuition behind Recurrent Neural Networks and LSTM\n",
        " 7.   Build and train the deep learning model\n",
        " 8.   Access the performance of the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VulXquoOxhD"
      },
      "source": [
        "# TASK #2: IMPORT LIBRARIES AND DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRctnznPHizH"
      },
      "source": [
        "!pip install plotly\n",
        "!pip install --upgrade nbformat\n",
        "!pip install nltk\n",
        "!pip install spacy # spaCy is an open-source software library for advanced natural language processing\n",
        "!pip install WordCloud\n",
        "!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "# import keras\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "!pip install jupyterthemes\n",
        "from jupyterthemes import jtplot\n",
        "jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) \n",
        "# setting the style of the notebook to be monokai theme  \n",
        "# this line of code is important to ensure that we are able to see the x and y axes clearly\n",
        "# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QiTczEunJNx"
      },
      "source": [
        "# load the data\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=19uJBw5Ond5m8MrrqLnHsELiRTzg1nSXQ' -O Fake.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vIaay2z09JnBBSIKHmscctzraBvHC-iU' -O True.csv\n",
        "\n",
        "\n",
        "df_true = pd.read_csv(\"True.csv\")\n",
        "df_fake = pd.read_csv(\"Fake.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGmp_8iJHizR"
      },
      "source": [
        "MINI CHALLENGE #1: \n",
        "- Indicate how many data samples do we have per class (i.e.: Fake and True)\n",
        "- List how many Null element are present and the memory usage for each dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8QB-ZRU1bsq"
      },
      "source": [
        "df_fake.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJxu6TTIcMwZ"
      },
      "source": [
        "df_fake.iloc[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqA0DkxR1hMd"
      },
      "source": [
        "df_true.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHmv5kzwHizR"
      },
      "source": [
        "df_true.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSF1R2Jm1DEV"
      },
      "source": [
        "df_fake.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MwD-8Or1F2Y"
      },
      "source": [
        "df_true.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHvpqGf91I__"
      },
      "source": [
        "df_fake.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s02rKlXh1Ppb"
      },
      "source": [
        "df_true.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6krkT9cF1UNc"
      },
      "source": [
        "df_fake.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBqxz1TBPBLE"
      },
      "source": [
        "# TASK #3: PERFORM EXPLORATORY DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IxC_95sovyJ"
      },
      "source": [
        "# add a target class column to indicate whether the news is real or fake\n",
        "df_true['isfake'] = 0\n",
        "df_true.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll5weq9qpGQO"
      },
      "source": [
        "df_fake['isfake'] = 1\n",
        "df_fake.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgTEJ5aGpHfU"
      },
      "source": [
        "# Concatenate Real and Fake News\n",
        "df = pd.concat([df_true, df_fake]).reset_index(drop = True)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dACVFu6zHizl"
      },
      "source": [
        "df.drop(columns = ['date'], inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUAF66vVqO3M"
      },
      "source": [
        "# combine title and text together\n",
        "df['original'] = df['title'] + ' ' + df['text']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8lK6fkQTmgs"
      },
      "source": [
        "df['original'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgkZHO3CPnsp"
      },
      "source": [
        "# TASK #4: PERFORM DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDtZAF0l8qx4"
      },
      "source": [
        "# download stopwords\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlZjHcAb8t91"
      },
      "source": [
        "# Obtain additional stopwords from nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sl0PEtkKhfn"
      },
      "source": [
        "# Remove stopwords and remove words with 2 or less characters\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
        "            result.append(token)\n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhh3-a4tRgIR"
      },
      "source": [
        "# Apply the function to the dataframe\n",
        "df['clean'] = df['original'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rmdS4RWHi0B"
      },
      "source": [
        "# Show original news\n",
        "df['original'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_eqTqT7RmSY"
      },
      "source": [
        "# Show cleaned up news after removing stopwords\n",
        "print(df['clean'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZapaeSOzHi0I"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otIh6BSiR5eG"
      },
      "source": [
        "# Obtain the total words present in the dataset\n",
        "list_of_words = []\n",
        "for i in df.clean:\n",
        "    for j in i:\n",
        "        list_of_words.append(j)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QthtVRnSHi0O"
      },
      "source": [
        "list_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldsj0RL_Hi0S"
      },
      "source": [
        "len(list_of_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKrrX-uETd9p"
      },
      "source": [
        "# Obtain the total number of unique words\n",
        "total_words = len(list(set(list_of_words)))\n",
        "total_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKfwnpbC6-i2"
      },
      "source": [
        "# join the words into a string\n",
        "df['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se9mTs5cHi0f"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPV9j8V3Xecm"
      },
      "source": [
        "df['clean_joined'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAfGGcaNfJAD"
      },
      "source": [
        "MINI CHALLENGE #2:\n",
        "- Perform sanity check on the prepocessing stage by visualizing at least 3 sample news \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inu5RDCUHi0k"
      },
      "source": [
        "df[['clean_joined','original']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf4bOLzfPc78"
      },
      "source": [
        "# TASK #5: VISUALIZE CLEANED UP DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUDiUeY7Hi0q"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbQz3U4DIFja"
      },
      "source": [
        "# plot the number of samples in 'subject'\n",
        "plt.figure(figsize = (8, 8))\n",
        "sns.countplot(y = \"subject\", data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ87W-QbHi0x"
      },
      "source": [
        "MINI CHALLENGE #3: \n",
        "- Plot the count plot for fake vs. true news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaH1vObGHi0x"
      },
      "source": [
        "plt.figure(figsize = (8, 8))\n",
        "sns.countplot(y = \"isfake\", data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTUOMF5nUnbC"
      },
      "source": [
        "# plot the word cloud for text that is Fake\n",
        "plt.figure(figsize = (20,20)) \n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined))\n",
        "plt.imshow(wc, interpolation = 'bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-1fy0se7Phi"
      },
      "source": [
        "# plot the word cloud for text that is True\n",
        "plt.figure(figsize = (20,20)) \n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 0].clean_joined))\n",
        "plt.imshow(wc, interpolation = 'bilinear')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r20ny06ECP1B"
      },
      "source": [
        "# length of maximum document will be needed to create word embeddings \n",
        "maxlen = -1\n",
        "for doc in df.clean_joined:\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    if(maxlen<len(tokens)):\n",
        "        maxlen = len(tokens)\n",
        "print(\"The maximum number of words in any document is =\", maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDtUZMRVcD7I"
      },
      "source": [
        "# visualize the distribution of number of words in a text\n",
        "import plotly.express as px\n",
        "fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins = 100)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi3neznbP_QT"
      },
      "source": [
        "# TASK #6: PREPARE THE DATA BY PERFORMING TOKENIZATION AND PADDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUgEf-SZ7R7c"
      },
      "source": [
        "# split data into test and train \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esCYLtieHi1E"
      },
      "source": [
        "from nltk import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kme-IYsM6uJa"
      },
      "source": [
        "# Create a tokenizer to tokenize the words and create sequences of tokenized words\n",
        "#fit_on_texts - will create a vocab\n",
        "#texts_to_sequence will create a encoded sequence\n",
        "\n",
        "tokenizer = Tokenizer(num_words = total_words)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWCpgXYNF1r"
      },
      "source": [
        "print(\"The encoding for document\\n\",df.clean_joined[0],\"\\n is : \",train_sequences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URy4Wkai_Qh3"
      },
      "source": [
        "# Add padding can either be maxlen = 4406 or smaller number maxlen = 40 seems to work well based on results\n",
        "padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\n",
        "padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D60mkoZvSG5D"
      },
      "source": [
        "for i,doc in enumerate(padded_train[:2]):\n",
        "     print(\"The padded encoding for document\",i+1,\" is : \",doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuhYli9FQN1Q"
      },
      "source": [
        "# TASK #7: UNDERSTAND THE THEORY AND INTUITION BEHIND RECURRENT NEURAL NETWORKS AND LSTM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuJGt0NnCz4z"
      },
      "source": [
        "RNN is for shorter sentences otherwise use LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-niuYZ8Hi1U"
      },
      "source": [
        "# TASK #8: UNDERSTAND THE INTUITION BEHIND LONG SHORT TERM MEMORY (LSTM) NETWORKS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4E8ovS8C4im"
      },
      "source": [
        "LSTM overcomes issue of vanishing gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHckYpNqQaMf"
      },
      "source": [
        "# TASK #9: BUILD AND TRAIN THE MODEL "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWRpgqzHHi1W"
      },
      "source": [
        "Embedding layer learns low dimension continuous representation of discrete values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGWlgFzrDzqm"
      },
      "source": [
        "Time Distributed Dense applies the same dense layer to every time step during GRU/LSTM Cell unrolling. That’s why the error function will be between the predicted label sequence and the actual label sequence.\n",
        "\n",
        "Using return_sequences=False, the Dense layer will get applied only once in the last cell. This is normally the case when RNNs are used for classification problems. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0d7qNMLUCc3"
      },
      "source": [
        "# Sequential Model\n",
        "model = Sequential()\n",
        "\n",
        "# embeddidng layer\n",
        "model.add(Embedding(total_words, output_dim = 128))\n",
        "# model.add(Embedding(total_words, output_dim = 240))\n",
        "\n",
        "\n",
        "# Bi-Directional RNN and LSTM\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "# Dense layers\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(1,activation= 'sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3tKg8McHi1Z"
      },
      "source": [
        "total_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzihy6KhHi1c"
      },
      "source": [
        "y_train = np.asarray(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEdds6i0-ohn"
      },
      "source": [
        "# train the model\n",
        "model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXa2MqEQQ2ww"
      },
      "source": [
        "# TASK #9: ASSESS TRAINED MODEL PERFORMANCE\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qRUkys-BSuQ"
      },
      "source": [
        "# make prediction\n",
        "pred = model.predict(padded_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh66RfZgF7ln"
      },
      "source": [
        "# if the predicted value is >0.5 it is real else it is fake\n",
        "prediction = []\n",
        "for i in range(len(pred)):\n",
        "    if pred[i].item() > 0.5:\n",
        "        prediction.append(1)\n",
        "    else:\n",
        "        prediction.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5pFJ_tMHi1t"
      },
      "source": [
        "# getting the accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(list(y_test), prediction)\n",
        "\n",
        "print(\"Model Accuracy : \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rpgzbSqHfR4"
      },
      "source": [
        "# get the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(list(y_test), prediction)\n",
        "plt.figure(figsize = (25, 25))\n",
        "sns.heatmap(cm, annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}